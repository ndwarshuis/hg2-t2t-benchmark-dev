from pathlib import Path
from os.path import dirname

src_dir = Path("resources")
src_query_dir = src_dir / "query"
src_bench_dir = src_dir / "bench"

res_dir = Path("results")

tools_dir = res_dir / "tools"

res_inter_dir = res_dir / "intermediate"
res_asm_dir = res_inter_dir / "asm"
res_hg2_dir = res_asm_dir / "hg2"
res_hg38_dir = res_asm_dir / "hg38"
res_dip_dir = res_inter_dir / "dipcall"
res_query_dir = res_inter_dir / "query"
res_bench_dir = res_inter_dir / "bench"
res_comp_dir = res_inter_dir / "comparison"
res_compbed_dir = res_comp_dir / "bed"

final_dir = res_dir / "final"
res_proj_dir = final_dir / "projections"

HAPLOTYPES = ["pat", "mat"]


################################################################################
# download a bunch of stuff


rule clone_liftover_scripts:
    output:
        directory(res_dir / "tools/liftover"),
    shell:
        """
        git clone \
        --depth 1 \
        --branch v0.2.0 \
        https://github.com/mobinasri/flagger.git \
        {output}
        """


# not compressed :(
rule download_hg2_asm:
    output:
        src_dir / "asm" / "hg2.fa.gz",
    params:
        url="https://s3-us-west-2.amazonaws.com/human-pangenomics/T2T/HG002/assemblies/hg002v0.9.fasta",
    conda:
        "envs/dipcall.yml"
    shell:
        "curl -Ss -q -L {params.url} | bgzip -c > {output}"


rule download_ref:
    output:
        src_dir / "references" / "GRCh38.fa.gz",
    params:
        url="https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz",
    shell:
        "curl -Ss -q -L {params.url} > {output}"


use rule download_ref as download_query_bed with:
    output:
        src_query_dir / "query.bed",
    params:
        url="https://giab-data.s3.amazonaws.com/defrabb_runs/20221101_v0.010-HG002-SV/results/draft_benchmarksets/GRCh38_HG002-verrkoV1.1-V0.6-dipz2k_smvar-exclude/GRCh38_HG2-verrkoV1.1-V0.6_dipcall-z2k.benchmark.bed",


use rule download_ref as download_bench_vcf with:
    output:
        src_bench_dir / "bench.vcf.gz",
    params:
        url="https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38/SupplementaryFiles/HG002_GRCh38_1_22_v4.2.1_benchmark_phased_MHCassembly_StrandSeqANDTrio.vcf.gz",


use rule download_ref as download_bench_bed with:
    output:
        src_bench_dir / "bench.bed",
    params:
        url="https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed",


use rule download_ref as download_paftools with:
    output:
        tools_dir / "minimap" / "paftools.js",
    params:
        url="https://raw.githubusercontent.com/lh3/minimap2/16b8d50199d607ba20754aaf5a7d111b644b66c0/misc/paftools.js",


use rule download_ref as download_PAR with:
    output:
        src_dir / "references" / "hg38_par.bed",
    params:
        url="https://github.com/lh3/dipcall/blob/master/data/hs38.PAR.bed",


use rule download_ref as download_strats with:
    output:
        src_dir / "references" / "strats.tar.gz",
    params:
        url="https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/genome-stratifications/v3.1/v3.1-genome-stratifications-GRCh38.tar.gz",


################################################################################
# run dipcall


rule list_asm_chrs:
    input:
        rules.download_hg2_asm.output,
    output:
        res_hg2_dir / "chrs.txt",
    shell:
        "gunzip -c {input} | grep -E '(MAT|PAT)' | sed 's/>//' > {output}"


# for testing
rule list_test_asm_chrs:
    input:
        rules.list_asm_chrs.output,
    output:
        res_hg2_dir / "chrs_filtered.txt",
    shell:
        "grep -E '(chr21|chr22)' {input} > {output}"


rule filter_asm:
    input:
        fa=rules.download_hg2_asm.output,
        regions=rules.list_asm_chrs.output,
        # regions=rules.list_test_asm_chrs.output,
    output:
        res_hg2_dir / "{hap}.fa.gz",
    conda:
        "envs/dipcall.yml"
    params:
        HAP=lambda wildcards: wildcards.hap.upper(),
    shell:
        """
        samtools faidx {input.fa} \
        $(grep {params.HAP} {input.regions} | tr '\n' ' ') | \
        bgzip -c \
        > {output}
        """


# flip this for testing
rule unzip_ref:
    input:
        rules.download_ref.output,
    output:
        res_hg38_dir / "ref_filtered.fa",
    conda:
        "envs/dipcall.yml"
    shell:
        # "samtools faidx {input} chr21 chr22 > {output}"
        "gunzip -c {input} > {output}"


rule index_ref:
    input:
        rules.unzip_ref.output,
    output:
        rules.unzip_ref.output[0] + ".fai",
    conda:
        "envs/dipcall.yml"
    shell:
        """
        samtools faidx {input}
        """


rule run_dipcall:
    input:
        **{h: expand(rules.filter_asm.output, hap=h) for h in HAPLOTYPES},
        ref=rules.unzip_ref.output,
        ref_idx=rules.index_ref.output,
        par=rules.download_PAR.output,
    output:
        **{
            k: res_dip_dir / f"hg2.{x}"
            for k, x in [
                ("pat_paf", "hap1.paf.gz"),
                ("mat_paf", "hap2.paf.gz"),
                ("vcf", "dip.vcf.gz"),
                ("bed", "dip.bed"),
                ("make", "dip.make"),
            ]
        },
    conda:
        "envs/dipcall.yml"
    params:
        prefix=lambda _, output: re.sub("\.dip.*", "", output.make),
    log:
        res_dip_dir / "dip.log",
    threads: 20
    resources:
        mem_mb=256000,
    shell:
        """
        run-dipcall \
            -z 200000,10000 \
            -t 5 \
            -x {input.par} \
            {params.prefix} \
            {input.ref} \
            {input.pat} \
            {input.mat} \
            > {output.make}
        make -j4 -f {output.make}
        """


################################################################################
# run liftover


# for testing
rule filter_bench_vcf:
    input:
        rules.download_bench_vcf.output,
    output:
        res_bench_dir / "filtered.vcf.gz",
    conda:
        "envs/dipcall.yml"
    shell:
        """
        gunzip -c {input} | \
        grep -E '^(#|chr(21|22))' | \
        bgzip -c > {output}
        """


rule unzip_strats:
    input:
        rules.download_strats.output,
    output:
        res_hg38_dir
        / "strats"
        / "v3.1-GRCh38-stratifications-all-except-genome-specific-stratifications.tsv",
    params:
        dir=lambda _, output: dirname(output[0]),
    shell:
        """
        mkdir -p {params.dir} &&
        tar xzf {input} --directory {params.dir} --strip-components=1
        """


rule run_happy:
    input:
        refi=rules.index_ref.output,
        ref=rules.unzip_ref.output,
        # bench_vcf=rules.filter_bench_vcf.output,
        bench_vcf=rules.download_bench_vcf.output,
        bench_bed=rules.download_bench_bed.output,
        query_vcf=rules.run_dipcall.output.vcf,
        query_bed=rules.run_dipcall.output.bed,
        strats=rules.unzip_strats.output,
    output:
        res_comp_dir / "happy" / "happy.vcf.gz",
    params:
        prefix=lambda wildcards, output: str(output[0]).replace(".vcf.gz", ""),
    conda:
        "envs/happy.yml"
    log:
        "log/happy.log",
    threads: 8
    resources:
        mem_mb=64000,
    shell:
        """
        HGREF={input.ref} \
        hap.py \
        --engine xcmp \
        --verbose \
        --threads {threads} \
        --stratification {input.strats} \
        -f {input.bench_bed} \
        -o {params.prefix} \
        -T {input.query_bed} \
        {input.bench_vcf} {input.query_vcf} \
        > {log} 2>&1
        """


rule remove_happy_SVs:
    input:
        rules.run_happy.output,
    output:
        res_compbed_dir / "no_SV_happy.vcf.gz",
    conda:
        "envs/bcftools.yml"
    shell:
        """
        gunzip -c {input} | \
        awk 'function abs(v) {{return v < 0 ? -v : v}}
             function sv(a) {{return abs(length(a)-length($4)) <= 50}}
             {{ n = split($5, a, ",");
                if (sv(a[1]) && (n == 1 || sv(a[2])))
                {{ print $0 }}
             }}' | \
        bgzip -c > \
        {output}
        """


use rule remove_happy_SVs as remove_dipcall_SVs with:
    input:
        rules.run_dipcall.output.vcf,
    output:
        res_compbed_dir / "no_SV_dipcall.vcf.gz",


rule index_dipcall_vcf:
    input:
        rules.remove_dipcall_SVs.output,
    output:
        rules.remove_dipcall_SVs.output[0] + ".tbi",
    conda:
        "envs/dipcall.yml"
    shell:
        "tabix {input}"


use rule index_dipcall_vcf as index_happy_vcf with:
    input:
        rules.remove_happy_SVs.output,
    output:
        rules.remove_happy_SVs.output[0] + ".tbi",


# happy sadly will strip the phasing information from the output vcf, so add
# it back here
rule annotate_happy_vcf:
    input:
        dipcall=rules.remove_dipcall_SVs.output,
        dipcall_tbx=rules.index_dipcall_vcf.output,
        happy=rules.remove_happy_SVs.output,
        happy_tbx=rules.index_happy_vcf.output,
    output:
        res_compbed_dir / "annotated.vcf.gz",
    conda:
        "envs/bcftools.yml"
    shell:
        """
        bcftools merge {input.dipcall} {input.happy} | \
        grep -vP './\.:\.\t./\.:\.' | \
        bgzip -c \
         > {output}
        """


rule vcf_to_bed:
    input:
        rules.annotate_happy_vcf.output,
    output:
        res_compbed_dir / "all_errors.bed",
    shell:
        """
        gunzip -c {input} | \
        sed '/^#/d' | \
        sed '/UNK/d' | \
        awk 'OFS="\t" {{ print $1, $2-1, $2+length($4)-1, $4, $5, $8, $9, $10, $11, $12 }}' \
        > {output}
        """


rule filter_gt_errors:
    input:
        rules.vcf_to_bed.output,
    output:
        gt=res_compbed_dir / "gt_errors.bed",
        nogt=res_compbed_dir / "nogt_errors.bed",
    shell:
        """
        grep ':am:' {input} | sed 's/\t/;/g4' > {output.gt}
        grep -v ':am:' {input} > {output.nogt}
        """


rule filter_fp_errors:
    input:
        rules.filter_gt_errors.output.nogt,
    output:
        res_compbed_dir / "fp_errors.bed",
    params:
        col=10,
        label="FP",
    shell:
        """
        cat {input} | \
        awk 'match(${params.col}, /{params.label}/) {{print $0}}' | \
        sed 's/\t/;/g4' \
        > {output}
        """


use rule filter_fp_errors as filter_fn_errors with:
    input:
        rules.filter_gt_errors.output.nogt,
    output:
        res_compbed_dir / "fn_errors.bed",
    params:
        col=9,
        label="FN",


rule merge_gt_errors:
    input:
        rules.filter_gt_errors.output.gt,
    output:
        res_compbed_dir / "gt_errors_collapsed.bed",
    conda:
        "envs/bedtools.yml"
    shell:
        """
        cat {input} | \
        sed 's/\t/;/g4' | \
        bedtools merge -i stdin -c 4 -o collapse -delim '~' \
        > {output}
        """


use rule merge_gt_errors as merge_fp_errors with:
    input:
        rules.filter_fp_errors.output,
    output:
        res_compbed_dir / "fp_errors_collapsed.bed",


use rule merge_gt_errors as merge_fn_errors with:
    input:
        rules.filter_fn_errors.output,
    output:
        res_compbed_dir / "fn_errors_collapsed.bed",


def liftover_input(wildcards):
    l = wildcards.label
    if l == "fp":
        return rules.merge_fp_errors.output
    elif l == "fn":
        return rules.merge_fn_errors.output
    elif l == "gt":
        return rules.merge_gt_errors.output
    else:
        assert False, "where did you learn to type? (wrong label)"


def paf_input(wildcards):
    h = wildcards.hap
    if h == "pat":
        return rules.run_dipcall.output.pat_paf
    elif h == "mat":
        return rules.run_dipcall.output.mat_paf
    assert False, "are you typing with 3.14 fingers? (wrong hap)"


rule unzip_paf:
    input:
        paf_input,
    output:
        res_inter_dir / "paf" / "{hap}.paf",
    shell:
        "gunzip -c {input} > {output}"


rule run_liftover:
    input:
        paf=rules.unzip_paf.output,
        bed=liftover_input,
        tooldir=rules.clone_liftover_scripts.output,
    output:
        projectable=res_compbed_dir / "projectable_{hap}_{label}.bed",
        projected=res_compbed_dir / "projected_{hap}_{label}.bed",
    threads: 8
    resources:
        mem_mb=32000,
    shell:
        """
        python {input.tooldir}/programs/src/project_blocks_multi_thread.py \
        --mode ref2asm \
        --paf {input.paf} \
        --blocks {input.bed} \
        --outputProjectable {output.projectable} \
        --outputProjection {output.projected} \
        --threads {threads}
        """


rule format_projected:
    input:
        rules.run_liftover.output.projected,
    output:
        final_dir / "projected_{hap}_{label}_final.bed.gz",
    conda:
        "envs/r.yml"
    script:
        "scripts/R/format.R"


use rule format_projected as format_projectable with:
    input:
        rules.run_liftover.output.projectable,
    output:
        final_dir / "projectable_{hap}_{label}_final.bed.gz",


def get_phase(wildcards):
    run_map = {
        ("mat", "fp", "falsehet"): ("0|1", ".\/."),
        ("pat", "fp", "falsehet"): ("1|0", ".\/."),
        ("pat", "gt", "collapse"): ("1|1", "0|1"),
        ("mat", "gt", "collapse"): ("1|1", "1|0"),
        ("pat", "gt", "collapse_noton"): ("1|1", "1|0"),
        ("mat", "gt", "collapse_noton"): ("1|1", "0|1"),
        ("pat", "gt", "collapse_unknownhap"): ("1|1", "0\/1"),
        ("mat", "gt", "collapse_unknownhap"): ("1|1", "0\/1"),
        ("mat", "gt", "falsehetorv4error_12"): ("1|2", "1\/1"),
        ("pat", "gt", "falsehetorv4error_21"): ("2|1", "1\/1"),
        ("mat", "gt", "falsehetorv4error_10"): ("1|0", "1\/1"),
        ("pat", "gt", "falsehetorv4error_01"): ("0|1", "1\/1"),
        ("pat", "fn", "collapse"): (".\/.", "1|0"),
        ("mat", "fn", "collapse"): (".\/.", "0|1"),
        ("pat", "fn", "collapse_noton"): (".\/.", "0|1"),
        ("mat", "fn", "collapse_noton"): (".\/.", "1|0"),
        ("mat", "fn", "collapse_unknownhap"): (".\/.", "0\/1"),
        ("pat", "fn", "collapse_unknownhap"): (".\/.", "0\/1"),
        ("mat", "fn", "error_unknownhap_21or12"): (".\/.", "(2\/1\|1\/2)"),
        ("pat", "fn", "error_unknownhap_21or12"): (".\/.", "(2\/1\|1\/2)"),
    }
    return run_map[(wildcards.hap, wildcards.label, wildcards.filebase)]


rule filter_projected:
    input:
        rules.format_projected.output,
    output:
        final_dir / "projected_filtered_{filebase}_{hap}_{label}.bed.gz",
    params:
        query_gt=lambda w: get_phase(w)[0],
        bench_gt=lambda w: get_phase(w)[1],
    shell:
        """
        gunzip -c {input} | \
        sed -n '/{params.query_gt}:bench_phase={params.bench_gt}/p' | \
        gzip -c > {output}
        """


all_runs = [
    ("mat", "fp", "falsehet"),
    ("pat", "fp", "falsehet"),
    ("pat", "gt", "collapse"),
    ("mat", "gt", "collapse"),
    ("pat", "gt", "collapse_noton"),
    ("mat", "gt", "collapse_noton"),
    ("pat", "gt", "collapse_unknownhap"),
    ("mat", "gt", "collapse_unknownhap"),
    ("mat", "gt", "falsehetorv4error_12"),
    ("pat", "gt", "falsehetorv4error_21"),
    ("mat", "gt", "falsehetorv4error_10"),
    ("pat", "gt", "falsehetorv4error_01"),
    ("pat", "fn", "collapse"),
    ("mat", "fn", "collapse"),
    ("pat", "fn", "collapse_noton"),
    ("mat", "fn", "collapse_noton"),
    ("mat", "fn", "collapse_unknownhap"),
    ("pat", "fn", "collapse_unknownhap"),
    ("mat", "fn", "error_unknownhap_21or12"),
    ("pat", "fn", "error_unknownhap_21or12"),
]

hap, label, filebase = map(list, zip(*all_runs))


rule all:
    input:
        expand(
            rules.filter_projected.output + rules.format_projectable.output,
            zip,
            filebase=filebase,
            label=label,
            hap=hap,
        ),
