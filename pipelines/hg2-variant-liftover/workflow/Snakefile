from pathlib import Path
from os.path import dirname

# query files from here: https://drive.google.com/drive/u/1/folders/1lqGkHjY7YL0Dvm-YWRpaIYDQILAo3P8K
# pafs from here: https://drive.google.com/drive/u/1/folders/12_Y5Gvx50g6t9JVsRWdXWMp6L9tb3jJj

src_dir = Path("resources")
src_query_dir = src_dir / "query"
src_bench_dir = src_dir / "bench"

res_dir = Path("results")

tools_dir = res_dir / "tools"

res_inter_dir = res_dir / "intermediate"
res_asm_dir = res_inter_dir / "asm"
res_hg2_dir = res_asm_dir / "hg2"
res_hg38_dir = res_asm_dir / "hg38"
res_dip_dir = res_inter_dir / "dipcall"
res_query_dir = res_inter_dir / "query"
res_bench_dir = res_inter_dir / "bench"
res_comp_dir = res_inter_dir / "comparison"
res_compbed_dir = res_comp_dir / "bed"

final_dir = res_dir / "final"
res_proj_dir = final_dir / "projections"

HAPLOTYPES = ["pat", "mat"]


################################################################################
# download a bunch of stuff


rule clone_liftover_scripts:
    output:
        directory(res_dir / "tools/liftover"),
    shell:
        """
        git clone \
        --depth 1 \
        --branch v0.2.0 \
        https://github.com/mobinasri/flagger.git \
        {output}
        """


# not compressed :(
rule download_hg2_asm:
    output:
        src_dir / "asm" / "hg2.fa.gz",
    params:
        url="https://s3-us-west-2.amazonaws.com/human-pangenomics/T2T/scratch/HG002/assemblies/drafts/assembly.v0.7.fasta",
    conda:
        "envs/dipcall.yml"
    shell:
        "curl -Ss -q -L {params.url} | bgzip -c > {output}"


rule download_ref:
    output:
        src_dir / "references" / "GRCh38.fa.gz",
    params:
        url="https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz",
    shell:
        "curl -Ss -q -L {params.url} > {output}"


use rule download_ref as download_query_bed with:
    output:
        src_query_dir / "query.bed",
    params:
        url="https://giab-data.s3.amazonaws.com/defrabb_runs/20221101_v0.010-HG002-SV/results/draft_benchmarksets/GRCh38_HG002-verrkoV1.1-V0.6-dipz2k_smvar-exclude/GRCh38_HG2-verrkoV1.1-V0.6_dipcall-z2k.benchmark.bed",


use rule download_ref as download_bench_vcf with:
    output:
        src_bench_dir / "bench.vcf.gz",
    params:
        url="https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38/SupplementaryFiles/HG002_GRCh38_1_22_v4.2.1_benchmark_phased_MHCassembly_StrandSeqANDTrio.vcf.gz",


use rule download_ref as download_bench_bed with:
    output:
        src_bench_dir / "bench.bed",
    params:
        url="https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed",


use rule download_ref as download_paftools with:
    output:
        tools_dir / "minimap" / "paftools.js",
    params:
        url="https://raw.githubusercontent.com/lh3/minimap2/16b8d50199d607ba20754aaf5a7d111b644b66c0/misc/paftools.js",


use rule download_ref as download_PAR with:
    output:
        src_dir / "references" / "hg38_par.bed",
    params:
        url="https://github.com/lh3/dipcall/blob/master/data/hs38.PAR.bed",


use rule download_ref as download_strats with:
    output:
        src_dir / "references" / "strats.tar.gz",
    params:
        url="https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/genome-stratifications/v3.1/v3.1-genome-stratifications-GRCh38.tar.gz",


################################################################################
# run dipcall


rule list_asm_chrs:
    input:
        rules.download_hg2_asm.output,
    output:
        res_hg2_dir / "chrs.txt",
    shell:
        "gunzip -c {input} | grep -E '(MAT|PAT)' | sed 's/>//' > {output}"


# for testing
rule list_test_asm_chrs:
    input:
        rules.list_asm_chrs.output,
    output:
        res_hg2_dir / "chrs_filtered.txt",
    shell:
        "grep -E '(chr21|chr22)' {input} > {output}"


rule filter_asm:
    input:
        fa=rules.download_hg2_asm.output,
        # regions=rules.list_asm_chrs.output,
        regions=rules.list_test_asm_chrs.output,
    output:
        res_hg2_dir / "{hap}.fa.gz",
    conda:
        "envs/dipcall.yml"
    params:
        HAP=lambda wildcards: wildcards.hap.upper(),
    shell:
        """
        samtools faidx {input.fa} \
        $(grep {params.HAP} {input.regions} | tr '\n' ' ') | \
        bgzip -c \
        > {output}
        """


rule unzip_ref:
    input:
        rules.download_ref.output,
    output:
        res_hg38_dir / "ref_filtered.fa",
    conda:
        "envs/dipcall.yml"
    shell:
        "samtools faidx {input} chr21 chr22 > {output}"
        # "gunzip -c {input} > {output}"


rule index_ref:
    input:
        rules.unzip_ref.output,
    output:
        rules.unzip_ref.output[0] + ".fai",
    conda:
        "envs/dipcall.yml"
    shell:
        """
        samtools faidx {input}
        """


rule run_dipcall:
    input:
        **{h: expand(rules.filter_asm.output, hap=h) for h in HAPLOTYPES},
        ref=rules.unzip_ref.output,
        ref_idx=rules.index_ref.output,
        par=rules.download_PAR.output,
    output:
        **{
            k: res_dip_dir / f"hg2.{x}"
            for k, x in [
                ("pat_paf", "hap1.paf.gz"),
                ("mat_paf", "hap2.paf.gz"),
                ("vcf", "dip.vcf.gz"),
                ("bed", "dip.bed"),
                ("make", "dip.make"),
            ]
        },
    conda:
        "envs/dipcall.yml"
    params:
        prefix=lambda _, output: re.sub("\.dip.*", "", output.make),
    log:
        res_dip_dir / "dip.log",
    threads: 20
    resources:
        mem_mb=256000,
    shell:
        """
        run-dipcall \
            -z 200000,10000 \
            -t 5 \
            -x {input.par} \
            {params.prefix} \
            {input.ref} \
            {input.pat} \
            {input.mat} \
            > {output.make}
        make -j4 -f {output.make}
        """


################################################################################
# run liftover


# for testing
rule filter_bench_vcf:
    input:
        rules.download_bench_vcf.output,
    output:
        res_bench_dir / "filtered.vcf.gz",
    conda:
        "envs/dipcall.yml"
    shell:
        """
        gunzip -c {input} | \
        grep -E '^(#|chr(21|22))' | \
        bgzip -c > {output}
        """


rule unzip_strats:
    input:
        rules.download_strats.output,
    output:
        res_hg38_dir
        / "strats"
        / "v3.1-GRCh38-stratifications-all-except-genome-specific-stratifications.tsv",
    params:
        dir=lambda _, output: dirname(output[0]),
    shell:
        """
        mkdir -p {params.dir} &&
        tar xzf {input} --directory {params.dir} --strip-components=1
        """


rule run_happy:
    input:
        refi=rules.index_ref.output,
        ref=rules.unzip_ref.output,
        bench_vcf=rules.filter_bench_vcf.output,
        bench_bed=rules.download_bench_bed.output,
        query_vcf=rules.run_dipcall.output.vcf,
        query_bed=rules.run_dipcall.output.bed,
        strats=rules.unzip_strats.output,
    output:
        res_comp_dir / "happy" / "happy.vcf.gz",
    params:
        prefix=lambda wildcards, output: str(output[0]).replace(".vcf.gz", ""),
    conda:
        "envs/happy.yml"
    log:
        "log/happy.log",
    threads: 8
    resources:
        mem_mb=64000,
    shell:
        """
        HGREF={input.ref} \
        hap.py \
        --engine xcmp \
        --verbose \
        --threads {threads} \
        --stratification {input.strats} \
        -f {input.bench_bed} \
        -o {params.prefix} \
        -T {input.query_bed} \
        {input.bench_vcf} {input.query_vcf} \
        > {log} 2>&1
        """


rule index_dipcall_vcf:
    input:
        rules.run_dipcall.output.vcf,
    output:
        rules.run_dipcall.output.vcf + ".tbi",
    conda:
        "envs/dipcall.yml"
    shell:
        "tabix {input}"


use rule index_dipcall_vcf as index_happy_vcf with:
    input:
        rules.run_happy.output,
    output:
        rules.run_happy.output[0] + ".tbi",


# happy sadly will strip the phasing information from the output vcf, so add
# it back here
rule annotate_happy_vcf:
    input:
        dipcall=rules.run_dipcall.output.vcf,
        dipcall_tbx=rules.index_dipcall_vcf.output,
        happy=rules.run_happy.output,
        happy_tbx=rules.index_happy_vcf.output,
    output:
        res_compbed_dir / "annotated.vcf.gz",
    conda:
        "envs/bcftools.yml"
    shell:
        """
        bcftools merge {input.dipcall} {input.happy} | \
        grep -v './\.:\.    ./\.:\.'  | \
        bgzip -c \
         > {output}
        """


rule vcf_to_bed:
    input:
        rules.annotate_happy_vcf.output,
    output:
        res_compbed_dir / "all_errors.bed",
    shell:
        """
        gunzip -c {input} | \
        sed '/^#/d' | \
        sed '/UNK/d' | \
        awk 'OFS="\t" {{ print $1, $2-1, $2+length($4)-1, $4, $5, $8, $9, $10, $11, $12 }}' \
        > {output}
        """


rule filter_gt_errors:
    input:
        rules.vcf_to_bed.output,
    output:
        gt=res_compbed_dir / "gt_errors.bed",
        nogt=res_compbed_dir / "nogt_errors.bed",
    shell:
        """
        grep ':am:' {input} | sed 's/\t/;/g4' > {output.gt}
        grep -v ':am:' {input} > {output.nogt}
        """


rule filter_fp_errors:
    input:
        rules.filter_gt_errors.output.nogt,
    output:
        res_compbed_dir / "fp_errors.bed",
    params:
        col=10,
        label="FP",
    shell:
        """
        cat {input} | \
        awk 'match(${params.col}, /{params.label}/) {{print $0}}' | \
        sed 's/\t/;/g4' \
        > {output}
        """


use rule filter_fp_errors as filter_fn_errors with:
    input:
        rules.filter_gt_errors.output.nogt,
    output:
        res_compbed_dir / "fn_errors.bed",
    params:
        col=9,
        label="FN",


rule merge_gt_errors:
    input:
        rules.filter_gt_errors.output.gt,
    output:
        res_compbed_dir / "gt_errors_collapsed.bed",
    conda:
        "envs/bedtools.yml"
    shell:
        """
        cat {input} | \
        sed 's/\t/;/g4' | \
        bedtools merge -i stdin -c 4 -o collapse -delim '~' \
        > {output}
        """


use rule merge_gt_errors as merge_fp_errors with:
    input:
        rules.filter_fp_errors.output,
    output:
        res_compbed_dir / "fp_errors_collapsed.bed",


use rule merge_gt_errors as merge_fn_errors with:
    input:
        rules.filter_fn_errors.output,
    output:
        res_compbed_dir / "fn_errors_collapsed.bed",


def liftover_input(wildcards):
    l = wildcards.label
    if l == "fp":
        return rules.merge_fp_errors.output
    elif l == "fn":
        return rules.merge_fn_errors.output
    elif l == "gt":
        return rules.merge_gt_errors.output
    else:
        assert False, "where did you learn to type? (wrong label)"


def paf_input(wildcards):
    h = wildcards.hap
    if h == "pat":
        return rules.run_dipcall.output.pat_paf
    elif h == "mat":
        return rules.run_dipcall.output.mat_paf
    assert False, "are you typing with 3.14 fingers? (wrong hap)"


rule unzip_paf:
    input:
        paf_input,
    output:
        res_inter_dir / "paf" / "{hap}.paf",
    shell:
        "gunzip -c {input} > {output}"


rule run_liftover:
    input:
        paf=rules.unzip_paf.output,
        bed=liftover_input,
        tooldir=rules.clone_liftover_scripts.output,
    output:
        projectable=final_dir / "projectable_{hap}_{label}.bed",
        projected=final_dir / "projected_{hap}_{label}.bed",
    threads: 8
    resources:
        mem_mb=32000,
    shell:
        """
        python {input.tooldir}/programs/src/project_blocks_multi_thread.py \
        --mode ref2asm \
        --paf {input.paf} \
        --blocks {input.bed} \
        --outputProjectable {output.projectable} \
        --outputProjection {output.projected} \
        --threads {threads}
        """


rule all:
    input:
        expand(
            rules.run_liftover.output,
            hap=["mat", "pat"],
            label=["fp", "fn", "gt"],
        ),


################################################################################
# rule graveyard
#
# use rule download_query_vcf as download_paf with:
#     output:
#         src_dir / "paf" / "{hap}.bam.gz",
#     params:
#         url=paf_url,
