from pathlib import Path

# query files from here: https://drive.google.com/drive/u/1/folders/1lqGkHjY7YL0Dvm-YWRpaIYDQILAo3P8K
# pafs from here: https://drive.google.com/drive/u/1/folders/12_Y5Gvx50g6t9JVsRWdXWMp6L9tb3jJj

src_dir = Path("resources")
src_query_dir = src_dir / "query"
src_bench_dir = src_dir / "bench"

res_dir = Path("results")

tools_dir = res_dir / "tools"

res_inter_dir = res_dir / "intermediate"
res_paf_dir = res_inter_dir / "paf"
res_query_dir = res_inter_dir / "query"
res_comp_dir = res_inter_dir / "comparison"
res_compbed_dir = res_comp_dir / "bed"
res_proj_dir = res_inter_dir / "projections"

final_dir = res_dir / "final"


rule download_query_vcf:
    output:
        src_query_dir / "query.vcf.gz",
    params:
        url="https://giab-data.s3.amazonaws.com/defrabb_runs/20221101_v0.010-HG002-SV/results/asm_varcalls/GRCh38_HG002-verrkoV1.1-V0.6-dipz2k/GRCh38_HG2-verrkoV1.1-V0.6_dipcall-z2k.dip.vcf.gz",
    shell:
        "curl -Ss -q -L -o {output} {params.url}"


use rule download_query_vcf as download_query_bed with:
    output:
        src_query_dir / "query.bed",
    params:
        url="https://giab-data.s3.amazonaws.com/defrabb_runs/20221101_v0.010-HG002-SV/results/draft_benchmarksets/GRCh38_HG002-verrkoV1.1-V0.6-dipz2k_smvar-exclude/GRCh38_HG2-verrkoV1.1-V0.6_dipcall-z2k.benchmark.bed",


def paf_url(wildcards):
    h = wildcards.hap
    if h == "pat":
        return "https://giab-data.s3.amazonaws.com/defrabb_runs/20221101_v0.010-HG002-SV/results/asm_varcalls/GRCh38_HG002-verrkoV1.1-V0.6-dipz2k/GRCh38_HG2-verrkoV1.1-V0.6_dipcall-z2k.hap1.paf.gz"
    elif h == "mat":
        return "https://giab-data.s3.amazonaws.com/defrabb_runs/20221101_v0.010-HG002-SV/results/asm_varcalls/GRCh38_HG002-verrkoV1.1-V0.6-dipz2k/GRCh38_HG2-verrkoV1.1-V0.6_dipcall-z2k.hap2.paf.gz"
    assert False, "are you typing with 3.14 fingers? (wrong hap)"


use rule download_query_vcf as download_paf with:
    output:
        src_dir / "paf" / "{hap}.paf.gz",
    params:
        url=paf_url,


use rule download_query_vcf as download_bench_vcf with:
    output:
        src_bench_dir / "bench.vcf.gz",
    params:
        url="https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz",


use rule download_query_vcf as download_bench_bed with:
    output:
        src_bench_dir / "bench.bed",
    params:
        url="https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed",


use rule download_query_vcf as download_ref with:
    output:
        src_dir / "references" / "GRCh38.fa.gz",
    params:
        url="https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz",


rule unzip_ref:
    input:
        rules.download_ref.output,
    output:
        res_dir / "references" / "GRCh38.fa",
    shell:
        "gunzip -c {input} > {output}"


rule unzip_paf:
    input:
        rules.download_paf.output,
    output:
        res_paf_dir / "{hap}.paf",
    shell:
        "gunzip -c {input} > {output}"


rule index_ref:
    input:
        rules.unzip_ref.output,
    output:
        rules.unzip_ref.output[0] + ".fai",
    conda:
        "envs/samtools.yml"
    shell:
        "samtools faidx {input} -o {output}"


# for testing
rule filter_query_vcf:
    input:
        rules.download_query_vcf.output,
    output:
        res_query_dir / "filtered.vcf.gz",
    conda:
        "envs/samtools.yml"
    shell:
        """
        gunzip -c {input} | \
        grep -E '^(#|chr(21|22))' | \
        bgzip -c > {output}
        """


# for testing
rule filter_paf:
    input:
        rules.unzip_paf.output,
    output:
        res_paf_dir / "{hap}_filtered.paf",
    shell:
        "grep -E '^chr(21|22)' {input} > {output}"


# for testing
use rule filter_query_vcf as filter_bench_vcf with:
    input:
        rules.download_bench_vcf.output,
    output:
        res_dir / "filtered.vcf.gz",


rule run_happy:
    input:
        refi=rules.index_ref.output,
        ref=rules.unzip_ref.output,
        bench_vcf=rules.filter_bench_vcf.output,
        bench_bed=rules.download_bench_bed.output,
        query_vcf=rules.filter_query_vcf.output,
        query_bed=rules.download_query_bed.output,
    output:
        res_comp_dir / "happy" / "happy.vcf.gz",
    params:
        prefix=lambda wildcards, output: str(output[0]).replace(".vcf.gz", ""),
    conda:
        "envs/happy.yml"
    log:
        "log/happy.log",
    threads: 8
    shell:
        """
        HGREF={input.ref} \
        hap.py \
        --engine vcfeval \
        --verbose \
        --threads {threads} \
        -f {input.bench_bed} \
        -o {params.prefix} \
        -T {input.query_bed} \
        {input.bench_vcf} {input.query_vcf} \
        > {log} 2>&1
        """


rule vcf_to_bed:
    input:
        rules.run_happy.output,
    output:
        res_compbed_dir / "all_errors.bed",
    shell:
        """
        gunzip -c {input} | \
        sed '/^#/d' | \
        sed '/UNK/d' | \
        awk 'OFS="\t" {{ print $1, $2-1, $2+length($4)-1, $4, $5, $9, $10, $11 }}' \
        > {output}
        """


rule filter_gt_errors:
    input:
        rules.vcf_to_bed.output,
    output:
        gt=res_compbed_dir / "gt_errors.bed",
        nogt=res_compbed_dir / "nogt_errors.bed",
    shell:
        """
        grep ':am:' {input} | sed 's/\t/;/g4' > {output.gt}
        grep -v ':am:' {input} > {output.nogt}
        """


rule filter_fp_errors:
    input:
        rules.filter_gt_errors.output.nogt,
    output:
        res_compbed_dir / "fp_errors.bed",
    params:
        col=8,
        label="FP",
    shell:
        """
        cat {input} | \
        awk 'match(${params.col}, /{params.label}/) {{print $0}}' | \
        sed 's/\t/;/g4' \
        > {output}
        """


use rule filter_fp_errors as filter_fn_errors with:
    input:
        rules.filter_gt_errors.output.nogt,
    output:
        res_compbed_dir / "fn_errors.bed",
    params:
        col=7,
        label="FN",


def liftover_input(wildcards):
    l = wildcards.label
    if l == "fp":
        return rules.filter_fp_errors.output
    elif l == "fn":
        return rules.filter_fn_errors.output
    elif l == "gt":
        return rules.filter_gt_errors.output.gt
    else:
        assert False, "where did you learn to type? (wrong label)"


rule clone_liftover_scripts:
    output:
        directory(res_dir / "tools/liftover"),
    shell:
        """
        git clone \
        --depth 1 \
        --branch v0.2.0 \
        https://github.com/mobinasri/flagger.git \
        {output}
        """


# use rule download_query_vcf as download_paftools with:
#     output:
#         tools_dir / "minimap" / "paftools.js",
#     params:
#         url="https://github.com/lh3/minimap2/blob/16b8d50199d607ba20754aaf5a7d111b644b66c0/misc/paftools.js",


rule run_liftover:
    input:
        paf=rules.filter_paf.output,
        bed=liftover_input,
        tooldir=rules.clone_liftover_scripts.output,
    output:
        projectable=res_proj_dir / "projectable_{hap}_{label}.bed",
        projected=res_proj_dir / "projected_{hap}_{label}.bed",
    threads: 8
    shell:
        """
        python {input.tooldir}/programs/src/project_blocks_multi_thread.py \
        --mode ref2asm \
        --paf {input.paf} \
        --blocks {input.bed} \
        --outputProjectable {output.projectable} \
        --outputProjection {output.projected} \
        --threads {threads}
        """


rule unsquish_projections:
    input:
        rules.run_liftover.output.projected,
    output:
        final_dir / "projected_{hap}_{label}.bed",
    shell:
        "cat {input} | tr ';' '\t' > {output}"


# TODO if we want we can filter the lifted beds after the fact


rule all:
    input:
        expand(
            rules.unsquish_projections.output,
            hap=["mat", "pat"],
            label=["fp", "fn", "gt"],
        ),
